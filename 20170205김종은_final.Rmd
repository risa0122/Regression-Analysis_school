---
title: "20170205김종은_final"
author: "20170205_김종은"
date: "12/14/2020"
output: html_document
---

```{r}
#1.
#(1)
#귀무가설: Female 변수는 Sales(판매량)에 유의하지 않다(설명하지 못한다).
#대립가설:Female 변수는 Sales(판매량)에 유의하다(설명한다).

df <- read.table('/Users/jongeun/Documents/Regression/final_data/Cigarette_Consumption.txt',sep='\t',header=T)
summary(df)
head(df)

lm <- lm(Sales ~ Age+HS+Income+Black+Female+Price, data=df)
summary(lm)
#female 의 p-value는 0.85071이므로 5% 유의수준에서 귀무가설을 채택한다. 
# 즉, Female 변수는 Sales(판매량)에 유의하지 않다(설명하지 못한다).

#(2)
#귀무가설: Female 변수와 HS 변수는 Sales(판매량)에 유의하지 않다(설명하지 못한다).
#대립가설:Female 변수와 HS 변수는 Sales(판매량)에 유의하다(설명한다).

step=step(lm,direction="both")
#(전체 모형)Sales ~ Age + HS + Income + Black + Female + Price 의 AIC=346.99
#(Female 변수와 HS변수가 빠진 모형) Sales ~ Age + Income + Black + Price 의 AIC=343.04

#따라서 두 변수는 AIC를 최소화하는데 도움을 주지 못하기때문에 귀무가설: Female 변수와 HS 변수는 Sales(판매량)에 유의하지 않다(설명하지 못한다).채택


#(3)
confint(lm,level = 0.95)
# Income 의 95% 신뢰구간은 -1.642517e-03<Income <0.03953542 다.


#(4)
lm2 <- lm(Sales ~ Age+HS+Black+Female+Price, data=df)
summary(lm2)
#결정계수(R^2)는 예측변수들에 의해 설명되는 Y변동의  비율을나타냄
#따라서 Income을 제거하기 전/후 회귀 모형의 결정계수가 각각 0.3208, 0.2678 로 Income를 제서하며 모형의 설명력이 약 5.3% 감소한 것으로 보아 설명비율은 5.3%이다.

#(5)
lm3 <- lm(Sales~Income, data=df)
summary(lm3)
# Income만 있는회귀 모형의 결정계수가 0.1063 로  Sales의 변동에 대한 설명비율은 10.63%이다.
# (4)의 결과인 5.3%와는 다르다. 그 이유는 Income자체만의 모형에 대한 설명비율과/ Income을 포함한 모형에서 - 제거된 모형을 뺀 설명비율은 다르기 때문


#(6)
lm4 <- lm(Sales ~ Price+Age+Income, data=df)
summary(lm4)

#결정계수(R^2)는 예측변수들에 의해 설명되는 Y변동의  비율을 나타내고,결정계수가 0.3032이므로 Sales의 변동에 대한 설명비율은 30.32%이다. 

#(7)
#1) Cook’s distance
cooks.distance(lm)>0.5
cutoff = 4/(nrow(df)-length(lm$coefficients)-2) 
plot(lm, which=4, col=2, lwd=2, cook.levels = cutoff)
plot(cooks.distance(lm), main="Cook's distance")


#영향점은 Cook’s distance가 Di>0.5 (or Di>1)
#영향점: 없음
#Cook’s distance가 Di> 4 / (n-k-1) = 0.0148이면 이상치로 의심
#이상치: 12,29,30


#2) Hat matrix
influence.measures(lm)
plot(lm, which=5, lwd=2)
#영향점은 hii>0.5
# 2,9
#이상치
#이상치: 2,9,10,25,29,30

#3) DFFITSi
dffits(lm)
n <- nrow(df)
k <- length(lm$coefficients)-1
cv <- 2*sqrt(k/n)
plot(dffits(lm), ylab = "Standardized dfFits", xlab = "Index",  main = paste("Standardized DfFits, \n critical value = 2*sqrt(k/n) = +/-", round(cv,3))) +abline(h = cv, lty = 2) +abline(h = -cv, lty = 2)

```

$|2\sqrt{((k+1)/(n-k-1))}|=0.686$

```{r}
#영향점은 (위의 식) |dffits|>0.686
# 영향점: 2,9,12,29,30,45
dffits(lm)
#보통 DFFITS의 절대값이 1보다 클 때 이상치로 의심
#이상치:9,12,29,30


#(8)
library(car)
vif(lm)
# Population과 Employed가 모두 VIF < 3 으로(10이하) 다중공선성 문제는 괜찮은 것으로 보임

#(9)
Anova(lm, type=3)
# Anova함수에의한 개별Type3 유의성검정
# 통계적 유의수준 0.05에서 Price와 Income만 유의함
# 담배 한 개의 가격이 담배 소비에 가장 큰 영향을 주며 다음으로 인당 소득이 영향을 줌


#(10)
# 1)변수선택법
step=step(lm,direction="both")
summary(step)
#최적 적합모형 : Sales ~ Age + Income + Price
# Age, Income, Price 3변수로 구성된 모형
# 결정계수R^2 : 0.3032
extractAIC(step)  #AIC: 342.2919

# 2)이상점
#(1)Cook’s distance
cooks.distance(step)
cutoff = 4/(nrow(df)-length(step$coefficients)-2) 
plot(step, which=4, col=2, lwd=2, cook.levels = cutoff)
plot(cooks.distance(step), main="Cook's distance")
#영향점은 Cook’s distance가 Di>0.5 (or Di>1)
#영향점: 없음
#Cook’s distance가 Di> 4 / (n-k-1) = 0.0148이면 이상치로 의심
#이상치: 12,29,30

#(2) Hat matrix
influence.measures(step)
plot(step, which=5, lwd=2)
#영향점은 hii>0.5
# 영향점 없음
#이상치: 2,7,10,29,30,32,38

#(3) DFFITSi
dffits(step)
n <- nrow(df)
k <- length(step$coefficients)-1
cv <- 2*sqrt(k/n)
plot(dffits(step), ylab = "Standardized dfFits", xlab = "Index",  main = paste("Standardized DfFits, \n critical value = 2*sqrt(k/n) = +/-", round(cv,3))) +abline(h = cv, lty = 2) +abline(h = -cv, lty = 2)
```

$|2\sqrt{((k+1)/(n-k-1))}|=0.485$

```{r}

#영향점은 (위의 식) |dffits|>0.485
# 영향점: 9,12,29,30,45
#보통 DFFITS의 절대값이 1보다 클 때 이상치로 의심
#이상치: 30

# 3)다중공선성
# 전체 변수 고려한 다중공선성은 (8)번 답안으로 기재. 이번 문제에서는 단계적 변수 선택법 step에서 구한 축소 모형에 대한 다중공선성 계산하겠음. 
#colldiag은 버전 문제로 안됨
#(1)대신 다중공선성 통계량을 통해 다중공선성 계산
vif(step)
# Population과 Employed가 모두 VIF < 2 로(10이하) 다중공선성 문제는 괜찮은 것으로 보임
#(2) 고유값, 고유백터
mm=model.matrix(step)
eigen(cor(mm[,-1]))
#고유값(eigen value) < 0.05 가 존재하지않고 다중공선성 문제 없음.
#고유벡터 또한 0.9이상인 값이 없어 다중공선성 문제 없음.
summary(step, correlation = T)$correlation
#회귀모형의 절편과 Age의 회귀 계수의 상관관계가 높지만 절편 (intercept)은 변수들과의 관계가 아니므로 다중공선성에는 영향을 미 치지 않음
# 모든 회귀계수간의 상관성은 높지 않음

#4)모든 경우 비교 
library(leaps)
x = cbind(df$Age, df$HS, df$Income, df$Black, df$Female, df$Price)
lp = leaps(x,y=df$Sales, method="Cp")
lp
# Cp측면에서는 Age, HS, Income, Black, Female, Price 모든 변수를 포함한 모형이 최적으로 나타나지만 비현실적임
# Cp를 고려하여 평가한다면 Age, Income, Price의 step 모형이 됨. 다중공선성문제는 없음.
```

```{r}
#2
df <- read.table('/Users/jongeun/Documents/Regression/final_data/기말문제2번데이터.txt',sep='\t',header=T)

head(df)
#(1) 회귀모형의 가정
#1) 선형성
lm <- lm(Y ~ ., data=df)
summary(lm)
step=step(lm,direction="both");step
# 선형 회귀모델을 만들고 변수 선택법을 통과되므로, 선형성 만족

#2) 독립성
library(car)
lm_res <- residuals(lm)
durbinWatsonTest(lm_res)
plot(rstudent(lm),main="residuals")
#p-value = 0.8362>0.05로 독립성을 만족(귀무가설채택)


#3) 등분산성
par(mfrow=c(2,2))
plot(lm, which = 1)
#분산이 고르지 않고, 선에 약간 밀접하게 분포돼있기 때 때문에 등분산성을 만족하지 않는다.  

#4) 정규성
shapiro.test(lm_res)
# p-value = 0.8407 (>0.05)이므로 정규성이 있다. 


#=>즉, 등분산성만 만족하지 않는다. 



#(2)
#1) 잔차
plot(lm$residuals)
# 가운데에 몰려있는 잔차의 패턴이 존재하는 등 적합성이 낮음 
#2) 표준화잔차
plot(rstandard(lm), lwd=2, col=4,main="Standardized residuals")  +abline(h=0, lwd=2, lty=3, col=2)
#  (-2, 2)안에 잔차가 패턴없이 분포되며 적합성 만족
#3) 스튜던트화잔차
plot(rstudent(lm), lwd=2, col=4, main="Studentized residuals")+abline(h=0, lwd=2, lty=3, col=2)
#가운데에 몰려있는 잔차의 패턴이 존재하는 등 적합성이 낮음 

#(3)
#1) Cook’s distance
cooks.distance(lm)>0.5
cutoff = 4/(nrow(df)-length(lm$coefficients)-2) 
plot(lm, which=4, col=2, lwd=2, cook.levels = cutoff)
plot(cooks.distance(lm), main="Cook's distance")

#영향점은 Cook’s distance가 Di>0.5 (or Di>1)
#영향점: 없음
#Cook’s distance가 Di> 4 / (n-k-1) = 0.0148이면 이상치로 의심
#이상치: 17,34,38

#2) Hat matrix
influence.measures(lm)
plot(lm, which=5, lwd=2)
#영향점은 hii>0.5
# 영향점 없음
#이상치
#이상치: 3,5,7,8,15,38

#3) DFFITSi
dffits(lm)
n <- nrow(df)
k <- length(lm$coefficients)-1
cv <- 2*sqrt(k/n)
plot(dffits(lm), ylab = "Standardized dfFits", xlab = "Index",  main = paste("Standardized DfFits, \n critical value = 2*sqrt(k/n) = +/-", round(cv,3))) +abline(h = cv, lty = 2) +abline(h = -cv, lty = 2)

```

$|2\sqrt{((k+1)/(n-k-1))}|=0.775$

```{r}
#영향점은 (위의 식) |dffits|>0.775
# 영향점: 17,24,34,38
dffits(lm)
#보통 DFFITS의 절대값이 1보다 클 때 이상치로 의심
#이상치:38

#(4)
library(car)
vif(lm)
# 변수 X1,X2,X3,X6이 VIF > 10 로 다중공선성 문제가 심각한 것으로 보임

#(5)
Anova(lm, type=3)
# Anova함수에의한 개별Type3 유의성검정
# 통계적 유의수준 0.05에서 X4만 유의함
# X4만 Y에 크고 유일하게 영향을 줌

#(6)
# 1)변수선택법
step=step(lm,direction="both")
summary(step)
#최적 적합모형 : Y ~ X1 + X2 + X3 + X4
# X1, X2, X3, X4 :4변수로 구성된 모형
# 결정계수R^2 : 0.9551
extractAIC(step)  #AIC: 271.0205



# 2)이상점
#(1)Cook’s distance
cooks.distance(step)>0.5
cutoff = 4/(nrow(df)-length(step$coefficients)-2) 
plot(step, which=4, col=2, lwd=2, cook.levels = cutoff)
plot(cooks.distance(step), main="Cook's distance")
#영향점은 Cook’s distance가 Di>0.5 (or Di>1)
#영향점: 없음
#Cook’s distance가 Di> 4 / (n-k-1) = 0.0148이면 이상치로 의심
#이상치: 3,34,38

#(2) Hat matrix
influence.measures(step)
plot(step, which=5, lwd=2)
#영향점은 hii>0.5
# 영향점 없음
#이상치: 2,7,10,29,30,32,38

#(3) DFFITSi
n <- nrow(df)
k <- length(step$coefficients)-1
cv <- 2*sqrt(k/n)
plot(dffits(step), ylab = "Standardized dfFits", xlab = "Index",  main = paste("Standardized DfFits, \n critical value = 2*sqrt(k/n) = +/-", round(cv,3))) +abline(h = cv, lty = 2) +abline(h = -cv, lty = 2)
```

$|2\sqrt{((k+1)/(n-k-1))}|=0.632$

```{r}
abs(dffits(step))>0.632
#영향점은 (위의 식) |dffits|>0.632
# 영향점: 10,17,34,38
abs(dffits(step))>1
#보통 DFFITS의 절대값이 1보다 클 때 이상치로 의심
#이상치: 34,38

# 3)다중공선성
# 전체 변수 고려한 다중공선성은 (4)번 답안으로 기재. 이번 문제에서는 단계적 변수 선택법 step에서 구한 축소 모형에 대한 다중공선성 계산하겠음. 
#colldiag은 버전 문제로 안됨
#(1)대신 다중공선성 통계량을 통해 다중공선성 계산
vif(step)
# Population과 Employed가 모두 VIF < 2 로(10이하) 다중공선성 문제는 괜찮은 것으로 보임
# 앞에서 전체 변수(x1~x6)의 다중공선성에서 상당히 개선된 것으로 보임. 
#(2) 고유값, 고유백터
mm=model.matrix(step)
eigen(cor(mm[,-1]))$vectors
#고유값(eigen value) < 0.05 가 존재함 다중공선성 문제 있음.
#고유벡터 또한 0.9이상인 값이 있음 다중공선성 문제 있음.
summary(step, correlation = T)$correlation
#모든 회귀계수 간 상관성가 매우 높음(X3,X4), 다중공선성에는 영향을 미 치지 않음

#4)모든 경우 비교 
library(leaps)
x = cbind(df$X1, df$X2, df$X3, df$X4, df$X5, df$X6)
lp = leaps(x,y=df$Y, method="Cp")
lp
# Cp측면에서는 X2,X3,X4,X5 변수를 포함한 모형이 최적으로 나타남.




```

```{r}
#3
salary_data <- read.table('/Users/jongeun/Documents/Regression/final_data/Salary_survey.txt',sep='\t',header=T)
summary(salary_data)
head(salary_data)

#(1)
library(dummies)
df<-dummy.data.frame(salary_data)
df$d_E = as.numeric(df$E)
df$d_M =as.numeric(df$M)
head(df)


lm1 <- lm(S~d_E+d_M, data=df)
summary(lm1)
#교육 정도와 관리자 유무에 따라 급여가 결정된다.

#S = 12696.8 + 860.5 d_E + 6603.8 d_M

#where d_E= 1 if E=‘고등학교’, d_E= 2 if E=‘대학교’,d_E= 3 if E=‘대학원이상’ / d_M= 1 if M=‘관리자’, d_M= 2 if M=‘관리자 아님’

#if(조건)
# d_E=1,  S = 13557.3 + 6603.8d_M
# d_E=2,  S = 14417.8 + 6603.8d_M
# d_E=3,  S = 15278.3 + 6603.8d_M
# d_M=1,  S = 19300.6 + 860.5 d_E
# d_M=2,  S = 25904.4 + 860.5 d_E



#(2)
#귀무가설: E(education,교육) 변수는 Salary(급여)에 유의하지 않다(설명하지 못한다).
#대립가설:E(education,교육) 변수는 Salary(급여)에 유의하다(설명한다).
lm2 <- lm(S~E, data=df)
summary(lm2)
#p-value: 0.06016 >0.05 이므로 귀무가설 채택.
#즉, E(education,교육) 변수만으로는 Salary(급여)에 유의하지 않다(설명하지 못한다).

#(3)
#두 회귀직선의 절편과 기울기의 일치성(두 회귀모형의 일치)에 대한 가설
```


$H_0:\beta_0m+\beta_0f=\beta_0 \,and\, \beta_1m+\beta_1f=\beta_1 \quad vs \quad H_1:not H_0$



```{r}
lm3 <- lm(S~d_E, data=df)
summary(lm3)
# Y = 13904.7+1701.2 d_E(education)+e(error)

# d_E=1,  S = 15605.9
# d_E=2,  S = 17307.1
# d_E=3,  S = 19008.3

# 기울기가 모두 다르다. 

#(4)
lm4 = lm(S ~ d_E + d_M + d_E*d_M, data=df)  
summary(lm4)

#S = 15714.8 - 774.3 d_E - 162.7 d_M + 3378.2 d_E*d_M
#교호작용d_E:d_M의 pvalue가 0.00629<0.05 이므로 교호작용은 유의미하다.

library(dplyr)
df = df %>% mutate(type= ifelse(d_E==1 & d_M==1, 1, ifelse(d_E==1 & d_M==2, 2,
ifelse(d_E==2 & d_M==1, 3, 
ifelse(d_E==2 & d_M==2, 4, 
ifelse(d_E==3 & d_M==1, 5, 6))))))

sne_grid = seq(1,5, by=0.5)
y11 = lm4$coeff[[1]]+lm4$coeff[[2]]+lm4$coeff[[3]]+lm4$coeff[[4]]*sne_grid 
y10 = lm4$coeff[[1]]+lm4$coeff[[2]]+lm4$coeff[[4]]*sne_grid
y01 = lm4$coeff[[1]]+lm4$coeff[[3]]+lm4$coeff[[4]]*sne_grid
y00 = lm4$coeff[[1]]+lm4$coeff[[4]]*sne_grid

with(plot(S~d_E+d_M, pch=type, lwd=2, col=1:4, frame.plot=F, sub="suitability for education & management"), data=df)


#+lines(y11~sne_grid, lty=1, lwd=2, col=1)
#+lines(y10~sne_grid, lty=2, lwd=2, col=2)
#+lines(y01~sne_grid, lty=3, lwd=2, col=3)
#+lines(y00~sne_grid, lty=4, lwd=2, col=4)
#+legend("topright", c("seeding & stationary", "seeding & moving", "noseeding & stationary", "noseeding & moving"),lwd=2, pch=1:4, col=1:4, lty=1:4)
#교호작용이 있다.


```
